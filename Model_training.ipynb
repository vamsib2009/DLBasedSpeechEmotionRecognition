{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, models, datasets\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define a function to read images\n",
    "def read_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((224, 224))  # Resize the image to match ResNet's input size\n",
    "    img = np.array(img)\n",
    "    return img\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_folder, pickle_file, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.pickle_file = pickle_file\n",
    "        self.transform = transform\n",
    "        self.data = self.load_pickle_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.data.iloc[idx]['filename']\n",
    "        image = read_image(os.path.join(self.image_folder, image_filename + \".jpg\"))\n",
    "        phoneme_embedding = self.data.iloc[idx]['phoneme_embeddings']\n",
    "        emotion = int(self.data.iloc[idx]['emotion'])\n",
    "\n",
    "        if self.transform:\n",
    "            image = Image.fromarray(image)  # Convert numpy array to PIL Image\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, phoneme_embedding, emotion\n",
    "\n",
    "    def load_pickle_data(self):\n",
    "        with open(self.pickle_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "\n",
    "# Define the MultimodalModel class\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, num_classes, phoneme_input_size, hidden_size):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        \n",
    "        # Initialize pre-trained ResNet model\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Identity()  # Remove the final fully connected layer\n",
    "        \n",
    "        # Define BiLSTM layers for processing phoneme embeddings\n",
    "        self.bilstm1 = nn.LSTM(input_size=phoneme_input_size, hidden_size=hidden_size, batch_first=True, bidirectional=True)\n",
    "        lstm_output_size = hidden_size * 2  # Because it's bidirectional\n",
    "        \n",
    "        # Define attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=num_ftrs + lstm_output_size, num_heads=1)\n",
    "        \n",
    "        # Define dense and dropout layers\n",
    "        self.dense1 = nn.Linear(num_ftrs + lstm_output_size, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Add another BiLSTM layer\n",
    "        self.bilstm2 = nn.LSTM(input_size=256, hidden_size=hidden_size, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Add another dense layer\n",
    "        self.dense2 = nn.Linear(hidden_size * 2, 128)  # Output size can be adjusted as needed\n",
    "        \n",
    "        # Final output dense layer\n",
    "        self.dense3 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)  # Adjust kernel size if needed\n",
    "\n",
    "    def forward(self, image_input, phoneme_input):\n",
    "        # Process image input through ResNet\n",
    "        image_output = self.resnet(image_input)  # No need to convert to byte or float\n",
    "        \n",
    "        # Process phoneme input through first BiLSTM\n",
    "        lstm_output1, _ = self.bilstm1(phoneme_input)\n",
    "        # Extract the final hidden state\n",
    "        phoneme_output1 = lstm_output1[:, -1, :]\n",
    "        \n",
    "        # Combine ResNet and first BiLSTM outputs\n",
    "        combined_output = torch.cat((image_output, phoneme_output1), dim=1)\n",
    "        \n",
    "        # Pass the combined output through the first dense layer\n",
    "        output = F.relu(self.dense1(combined_output))\n",
    "        \n",
    "        # Add another BiLSTM layer after dense1\n",
    "        lstm_output2, _ = self.bilstm2(output.unsqueeze(1))\n",
    "        lstm_output2 = lstm_output2[:, -1, :]  # Consider only the last timestep output\n",
    "        \n",
    "        # Add another dense layer after the second BiLSTM\n",
    "        output = F.relu(self.dense2(lstm_output2))\n",
    "        output = self.dropout(output) #Remove this dropout later\n",
    "        \n",
    "        # Final output layer\n",
    "        final_output = self.dense3(output)\n",
    "        \n",
    "        # Apply pooling\n",
    "        output = output.unsqueeze(2)  # Add a channel dimension for 1D pooling\n",
    "        output = self.pooling(output).squeeze(2)  # Pool along the temporal dimension\n",
    "        \n",
    "        return final_output\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 3: Define DataLoader and Model\n",
    "batch_size = 8\n",
    "num_classes = 4  # Number of classes\n",
    "hidden_size = 32\n",
    "phoneme_input_size = 44\n",
    "\n",
    "# Load dataset\n",
    "image_folder = ''\n",
    "pickle_file = '' #contain phoneme embeddings\n",
    "\n",
    "# Augmentation and transformation for training data\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    #transforms.RandomResizedCrop(224, scale=(0.8, 1.0)), #Do Center crop\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
    ")\n",
    "\n",
    "# Transformation for validation and test data\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
    ")\n",
    "\n",
    "# Load dataset with augmentation for training and without augmentation for validation and test\n",
    "dataset = CustomDataset(image_folder, pickle_file, transform=train_transforms)\n",
    "\n",
    "# Perform train-validation-test split\n",
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "test_size = 0.1\n",
    "train_dataset, temp_dataset = train_test_split(dataset, train_size=train_size, test_size=(val_size + test_size), shuffle=True)\n",
    "val_dataset, test_dataset = train_test_split(temp_dataset, train_size=val_size / (val_size + test_size), test_size=test_size / (val_size + test_size), shuffle=True)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the multimodal model\n",
    "multimodal_model = MultimodalModel(num_classes, phoneme_input_size, hidden_size)\n",
    "multimodal_model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(multimodal_model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "#optimizer = optim.Adam(multimodal_model.parameters(), lr=0.0001)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    multimodal_model.train()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_loss = 0.0  # Initialize total loss\n",
    "    for images, phoneme_embeddings, labels in train_loader:\n",
    "        images = images.to(device)  # Move images to GPU\n",
    "        phoneme_embeddings = phoneme_embeddings.to(device)  # Move phoneme embeddings to GPU\n",
    "        labels = labels.to(device)  # Move labels to GPU\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = multimodal_model(images, phoneme_embeddings)\n",
    "        labels = labels - 1\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    train_accuracy = total_correct / total_samples\n",
    "    train_average_loss = total_loss / total_samples\n",
    "    train_accuracy = train_accuracy*100\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Accuracy: {train_accuracy:.4f}, Train Average Loss: {train_average_loss:.4f}\")\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    multimodal_model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        val_total_loss = 0.0  # Initialize total loss for validation set\n",
    "        class_accuracies = defaultdict(int)  # Store individual class accuracies\n",
    "        \n",
    "        # Initialize a dictionary to store the number of samples for each class\n",
    "        class_samples = defaultdict(int)\n",
    "\n",
    "        # Iterate through the validation set\n",
    "        for images, phoneme_embeddings, labels in val_loader:\n",
    "            images = images.to(device)  # Move images to GPU\n",
    "            phoneme_embeddings = phoneme_embeddings.to(device)  # Move phoneme embeddings to GPU\n",
    "            labels = labels.to(device)  # Move labels to GPU\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = multimodal_model(images, phoneme_embeddings)\n",
    "            # Calculate loss\n",
    "            labels = labels - 1\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_total_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update class_samples dictionary\n",
    "            for pred, label in zip(predicted, labels):\n",
    "                class_samples[label.item()] += 1\n",
    "                if pred == label:\n",
    "                    class_accuracies[label.item()] += 1\n",
    "\n",
    "        val_accuracy = total_correct / total_samples\n",
    "        val_average_loss = val_total_loss / total_samples\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Calculate weighted validation accuracy\n",
    "        weighted_accuracy = sum(class_accuracies[label] / class_samples[label] * acc for label, acc in class_accuracies.items())\n",
    "        weighted_accuracy = weighted_accuracy/4\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Weighted Validation Accuracy: {weighted_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "multimodal_model.eval()\n",
    "with torch.no_grad():\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    test_total_loss = 0.0  # Initialize total loss for test set\n",
    "    \n",
    "    # Store individual class accuracies\n",
    "    class_accuracies = defaultdict(int)  \n",
    "    \n",
    "    # Store the number of samples for each class\n",
    "    class_samples = defaultdict(int)  \n",
    "\n",
    "    for images, phoneme_embeddings, labels in test_loader:\n",
    "        images = images.to(device)  # Move images to GPU\n",
    "        phoneme_embeddings = phoneme_embeddings.to(device)  # Move phoneme embeddings to GPU\n",
    "        labels = labels.to(device)  # Move labels to GPU\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = multimodal_model(images, phoneme_embeddings)\n",
    "        \n",
    "        # Calculate loss\n",
    "        labels = labels - 1\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_total_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Update class_samples dictionary\n",
    "        for pred, label in zip(predicted, labels):\n",
    "            class_samples[label.item()] += 1\n",
    "            if pred == label:\n",
    "                class_accuracies[label.item()] += 1\n",
    "\n",
    "    test_accuracy = total_correct / total_samples\n",
    "    test_average_loss = test_total_loss / total_samples\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Calculate weighted test accuracy\n",
    "    weighted_accuracy = sum(class_accuracies[label] / class_samples[label] * acc for label, acc in class_accuracies.items())\n",
    "    weighted_accuracy = weighted_accuracy/4\n",
    "    print(f\"Weighted Test Accuracy: {weighted_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
